{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1C5ALtJGK3MuWtMB_XUbyXBJs6zOPAxal",
      "authorship_tag": "ABX9TyMZUklYYZC/mUsuopSVN7s5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhanu902/FoodieChat-Bot/blob/main/BOT_PREPROCESSING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KJ5y7GbBRkS0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKK0TB1C0Htz",
        "outputId": "91c37b56-7901-4274-f34a-f8311266f809"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def load_data(folder_path):\n",
        "  data = {}\n",
        "\n",
        "  for filename in sorted(os.listdir(folder_path)):\n",
        "    if filename.endswith('.json'):\n",
        "      with open(os.path.join(folder_path, filename), 'r') as f:\n",
        "        dialogues = json.load(f)\n",
        "        for dialogue in dialogues:\n",
        "          data[dialogue['dialogue_id']] = dialogue\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "base_path = '/content/drive/MyDrive/ChatBot/MultiWOZ_2.2'\n",
        "\n",
        "train_data = load_data(os.path.join(base_path, 'train'))\n",
        "val_data = load_data(os.path.join(base_path, 'dev'))\n",
        "test_data = load_data(os.path.join(base_path, 'test'))\n",
        "\n",
        "print(f\"Total dialogues loaded for Train: {len(train_data)}\")\n",
        "print(f\"Total dialogues loaded for Test: {len(test_data)}\")\n",
        "print(f\"Total dialogues loaded for Validation: {len(val_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Fwk_-S9lK0x",
        "outputId": "c662330e-5bba-41e1-ef29-2a96c849dbf5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Total dialogues loaded for Train: 8437\n",
            "Total dialogues loaded for Test: 1000\n",
            "Total dialogues loaded for Validation: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **General NLP Pre-Processing**"
      ],
      "metadata": {
        "id": "MfoWLypWzL-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text, remove_stopwords=True, do_lemmatize=True):\n",
        "  text = text.lower()\n",
        "\n",
        "  text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "\n",
        "  if remove_stopwords:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "  if do_lemmatize:\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "  return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "3mZwgn9dz_XA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Intent Classification PreProcessing**"
      ],
      "metadata": {
        "id": "KBYPpytpStCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def intents_pp(data, split):\n",
        "\n",
        "  ### ---- Capture Intent ----\n",
        "  intent_data = []\n",
        "\n",
        "  for dialogue in data.values():\n",
        "    for turn in dialogue[\"turns\"]:\n",
        "      if turn[\"speaker\"] == \"USER\":\n",
        "        for frame in turn.get(\"frames\", []):\n",
        "          intent = frame.get(\"state\", {}).get(\"active_intent\", \"NONE\")\n",
        "          if intent != \"NONE\":\n",
        "            original = turn[\"utterance\"]\n",
        "            cleaned = clean_text(original)\n",
        "            intent_data.append((original, cleaned, intent))\n",
        "\n",
        "  ### ---- Convert to DataFrame ----\n",
        "  DF_intents = pd.DataFrame(intent_data, columns=[\"original\", \"cleaned\", \"intent\"])\n",
        "  ### ---- Label Encode the Intent ----\n",
        "  DF_intents[\"intent_ID\"] = LabelEncoder().fit_transform(DF_intents[\"intent\"])\n",
        "\n",
        "  DF_intents.to_csv(f\"DF_intents_{split}.csv\", index=False)\n",
        "\n",
        "  return DF_intents"
      ],
      "metadata": {
        "id": "lYaNiy2yTNtp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NER PreProcessing/ Slot Filling**"
      ],
      "metadata": {
        "id": "fxcynodwS60z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "### ---- BIO Tagging ----\n",
        "\n",
        "def bio_tag_utt(utterance, slot_values):\n",
        "\n",
        "  tokens = word_tokenize(utterance.lower())\n",
        "  tags = [\"O\"] * len(tokens)\n",
        "\n",
        "  for slot, values in slot_values.items():\n",
        "    for value in values:\n",
        "      value_tokens = word_tokenize(value.lower())\n",
        "\n",
        "      for i in range(len(tokens) - len(value_tokens) + 1):\n",
        "        if tokens[i : i + len(value_tokens)] == value_tokens:\n",
        "          tags[i] = f\"B-{slot}\"\n",
        "\n",
        "          for j in range(1, len(value_tokens)):\n",
        "            tags[i+j] = f\"I-{slot}\"\n",
        "\n",
        "  return tokens, tags"
      ],
      "metadata": {
        "id": "yLsEixlvXZk9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ---- Apply to Dataset ----\n",
        "def ner_pp(data, split):\n",
        "\n",
        "  bio_samples = []\n",
        "\n",
        "  for dialogue in data.values():\n",
        "    for turn in dialogue[\"turns\"]:\n",
        "      if turn[\"speaker\"] == \"USER\":\n",
        "        for frame in turn.get(\"frames\", []):\n",
        "          slot_values = frame.get(\"state\", {}).get(\"slot_values\", {})\n",
        "\n",
        "          if slot_values:\n",
        "            utt = turn[\"utterance\"]\n",
        "            tokens, tags = bio_tag_utt(utt, slot_values)\n",
        "            bio_samples.append({\"tokens\": tokens, \"tags\": tags})\n",
        "\n",
        "  with open(f\"BIO_{split}.json\", \"w\") as f:\n",
        "    json.dump(bio_samples, f, indent=2)\n",
        "\n",
        "  return bio_samples"
      ],
      "metadata": {
        "id": "SUCLBrXYbE8q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DST PreProcessing**"
      ],
      "metadata": {
        "id": "mxBNz23CTB2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dst_pp(data, split):\n",
        "  dst_data = []\n",
        "\n",
        "  for dialogue in data.values():\n",
        "    history = []\n",
        "\n",
        "    for turn in dialogue[\"turns\"]:\n",
        "      speaker = turn[\"speaker\"]\n",
        "      utt = turn[\"utterance\"]\n",
        "      history.append(f\"{speaker.lower()}: {utt.strip()}\")\n",
        "\n",
        "      if speaker == \"USER\":\n",
        "        combined_history = \" \".join(history)\n",
        "        full_state = {}\n",
        "\n",
        "        for frame in turn.get(\"frames\", []):\n",
        "          slot_values = frame.get(\"state\", {}).get(\"slot_values\", {})\n",
        "\n",
        "          for slot, values in slot_values.items():\n",
        "            if values:\n",
        "              full_state[slot] = values[-1]\n",
        "\n",
        "        if full_state:\n",
        "          dst_data.append({\"history\": combined_history, \"belief_state\": full_state})\n",
        "\n",
        "  with open(f\"DST_{split}.json\", \"w\") as f:\n",
        "    json.dump(dst_data, f, indent=2)\n",
        "\n",
        "  return dst_data"
      ],
      "metadata": {
        "id": "QjykExC5kanJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Response Generation PreProcessing**"
      ],
      "metadata": {
        "id": "-VIwDyDIp6GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def response_pp(data, split):\n",
        "    response_data = []\n",
        "    for dialogue in data.values():\n",
        "        for i, turn in enumerate(dialogue[\"turns\"]):\n",
        "            if turn[\"speaker\"] == \"SYSTEM\" and i > 0:\n",
        "                user_turn = dialogue[\"turns\"][i-1]\n",
        "                input_components = []\n",
        "                for frame in user_turn.get(\"frames\", []):\n",
        "                    intent = frame.get(\"state\", {}).get(\"active_intent\", \"NONE\")\n",
        "                    if intent != \"NONE\":\n",
        "                        input_components.append(f\"intent={intent}\")\n",
        "                    slot_values = frame.get(\"state\", {}).get(\"slot_values\", {})\n",
        "                    for slot, values in slot_values.items():\n",
        "                        if values:\n",
        "                            input_components.append(f\"{slot}={values[-1]}\")\n",
        "                input_text = \" \".join(input_components)\n",
        "                output_text = turn[\"utterance\"]\n",
        "                if input_text and output_text:\n",
        "                    response_data.append({\"input\": input_text, \"output\": output_text})\n",
        "    with open(f\"response_{split}.json\", \"w\") as f:\n",
        "        json.dump(response_data, f, indent=2)\n",
        "    return response_data"
      ],
      "metadata": {
        "id": "YcHFur3JmQE-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DataSet Spliting**"
      ],
      "metadata": {
        "id": "VDpZ8D1FswOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splits = {\n",
        "    \"train\": train_data,\n",
        "    #\"val\": val_data,\n",
        "    #\"test\": test_data\n",
        "}\n",
        "\n",
        "for split_name, data in splits.items():\n",
        "    print(f\"🔄 Preprocessing {split_name.upper()}...\")\n",
        "    df_intent = intents_pp(data, split_name)\n",
        "    bio_data = ner_pp(data, split_name)\n",
        "    dst_data = dst_pp(data, split_name)\n",
        "    response_data = response_pp(data, split_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As2QRWXOslz7",
        "outputId": "76da99d6-b6e9-46c6-b5cb-2b906cf01b94"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Preprocessing TRAIN...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "\n",
        "# ---------- INTENT SPLIT ----------\n",
        "df = pd.read_csv(\"DF_intents_train.csv\")  # file written by intents_pp()\n",
        "\n",
        "X = df[\"cleaned\"]\n",
        "y = df[\"intent_ID\"]\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "pd.DataFrame({\"text\": X_train, \"label\": y_train}).to_csv(\"intent_train.csv\", index=False)\n",
        "pd.DataFrame({\"text\": X_val, \"label\": y_val}).to_csv(\"intent_val.csv\", index=False)\n",
        "pd.DataFrame({\"text\": X_test, \"label\": y_test}).to_csv(\"intent_test.csv\", index=False)\n",
        "\n",
        "# ---------- BIO SPLIT ----------\n",
        "with open(\"BIO_train.json\", \"r\") as f:\n",
        "    bio_samples = json.load(f)\n",
        "\n",
        "bio_train, bio_temp = train_test_split(bio_samples, test_size=0.3, random_state=42)\n",
        "bio_val, bio_test = train_test_split(bio_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "with open(\"bio_train.json\", \"w\") as f: json.dump(bio_train, f, indent=2)\n",
        "with open(\"bio_val.json\", \"w\") as f: json.dump(bio_val, f, indent=2)\n",
        "with open(\"bio_test.json\", \"w\") as f: json.dump(bio_test, f, indent=2)\n",
        "\n",
        "# ---------- DST SPLIT ----------\n",
        "with open(\"DST_train.json\", \"r\") as f:\n",
        "    dst_samples = json.load(f)\n",
        "\n",
        "dst_train, dst_temp = train_test_split(dst_samples, test_size=0.3, random_state=42)\n",
        "dst_val, dst_test = train_test_split(dst_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "with open(\"dst_train.json\", \"w\") as f: json.dump(dst_train, f, indent=2)\n",
        "with open(\"dst_val.json\", \"w\") as f: json.dump(dst_val, f, indent=2)\n",
        "with open(\"dst_test.json\", \"w\") as f: json.dump(dst_test, f, indent=2)\n",
        "\n",
        "# ---------- RESPONSE SPLIT ----------\n",
        "with open(\"response_train.json\", \"r\") as f:\n",
        "    response_samples = json.load(f)\n",
        "\n",
        "res_train, res_temp = train_test_split(response_samples, test_size=0.3, random_state=42)\n",
        "res_val, res_test = train_test_split(res_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "with open(\"response_train.json\", \"w\") as f: json.dump(res_train, f, indent=2)\n",
        "with open(\"response_val.json\", \"w\") as f: json.dump(res_val, f, indent=2)\n",
        "with open(\"response_test.json\", \"w\") as f: json.dump(res_test, f, indent=2)"
      ],
      "metadata": {
        "id": "LWXTLw4e-ZxI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DEBUG**"
      ],
      "metadata": {
        "id": "E2NAyDtm_k5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"intent_train.csv\")\n",
        "df_val = pd.read_csv(\"intent_val.csv\")\n",
        "df_test = pd.read_csv(\"intent_test.csv\")\n",
        "\n",
        "print(\"Train set shape:\", df_train.shape)\n",
        "print(\"Validation set shape:\", df_val.shape)\n",
        "print(\"Test set shape:\", df_test.shape)\n",
        "\n",
        "print(\"\\nSample rows: \")\n",
        "print(df_train.head())\n",
        "\n",
        "print(\"\\nLabel value counts: \")\n",
        "print(df_train[\"label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nFcuDf9_kVd",
        "outputId": "431c72d9-456a-490d-ee5e-b3066dd0563f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set shape: (38603, 2)\n",
            "Validation set shape: (8272, 2)\n",
            "Test set shape: (8273, 2)\n",
            "\n",
            "Sample rows: \n",
            "                                                text  label\n",
            "0                       look hotel east free parking      6\n",
            "1        would perfect get reservation monday people      1\n",
            "2                     get thhe phone number postcode      5\n",
            "3         yes phone number lan hong house restaurant      8\n",
            "4  thanks also looking restaurant serf lebanese f...      8\n",
            "\n",
            "Label value counts: \n",
            "label\n",
            "8     7541\n",
            "6     7378\n",
            "10    6716\n",
            "3     5693\n",
            "1     3086\n",
            "0     3082\n",
            "9     2586\n",
            "2     1686\n",
            "5      495\n",
            "7      334\n",
            "4        6\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"bio_train.json\", \"r\") as f:\n",
        "    bio_data = json.load(f)\n",
        "\n",
        "print(\"Total BIO samples:\", len(bio_data))\n",
        "print(\"Sample:\")\n",
        "print(bio_data[0])\n",
        "\n",
        "# Validate format\n",
        "assert all(len(entry[\"tokens\"]) == len(entry[\"tags\"]) for entry in bio_data), \"Mismatch between tokens and tags\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHi-fo5HBCyA",
        "outputId": "ca01248a-d149-467b-b538-204576c064a9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total BIO samples: 58107\n",
            "Sample:\n",
            "{'tokens': ['i', \"'m\", 'also', 'looking', 'for', 'a', 'cinema', 'to', 'visit', '?'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'B-attraction-type', 'O', 'O', 'O']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"dst_train.json\", \"r\") as f:\n",
        "    dst_data = json.load(f)\n",
        "\n",
        "print(\"Total DST samples:\", len(dst_data))\n",
        "print(\"Sample:\")\n",
        "print(dst_data[0])\n",
        "\n",
        "# Basic checks\n",
        "assert isinstance(dst_data[0][\"history\"], str)\n",
        "assert isinstance(dst_data[0][\"belief_state\"], dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAz4qB0EBSmS",
        "outputId": "a9e53057-96f5-4a29-b922-b8ffb85c7743"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total DST samples: 38249\n",
            "Sample:\n",
            "{'history': 'user: Find a budget hotel with free parking in Cambridge. system: There are 10 hotels that meet your needs. Would you like to narrow your search by area? user: I would like it to have a four star rating and be located on the west side. system: The Cambridge Belfry fits your requirements, would you like to book a reservation there? user: Please book it for 5 people and 5 nights starting from wednesday.', 'belief_state': {'hotel-area': 'west', 'hotel-bookday': 'wednesday', 'hotel-bookpeople': '5', 'hotel-bookstay': '5', 'hotel-name': 'cambridge belfry', 'hotel-parking': 'yes', 'hotel-pricerange': 'cheap', 'hotel-stars': '4'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"response_train.json\", \"r\") as f:\n",
        "    res_data = json.load(f)\n",
        "\n",
        "print(\"Total response pairs:\", len(res_data))\n",
        "print(\"Sample:\")\n",
        "print(res_data[0])\n",
        "\n",
        "assert \"input\" in res_data[0] and \"output\" in res_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wy5AuONBU0R",
        "outputId": "f54a91c7-39d3-471b-d696-3a37c5ba5621"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total response pairs: 39404\n",
            "Sample:\n",
            "{'input': 'intent=book_hotel hotel-bookday=sunday hotel-bookpeople=5 hotel-bookstay=3 hotel-name=ashley hotel attraction-name=cherry hinton hall and grounds attraction-type=entertainment', 'output': 'I was able to book it, reference number 6BIQ6UWS'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Save**"
      ],
      "metadata": {
        "id": "17Twidg1Ccsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📂 Define your target folder path\n",
        "preprocessed_folder = '/content/drive/MyDrive/ChatBot/Preprocessed'\n",
        "\n",
        "# ✅ Create the folder if it doesn’t exist\n",
        "os.makedirs(preprocessed_folder, exist_ok=True)\n",
        "\n",
        "# ---------------------- INTENT ---------------------- #\n",
        "# Save intent train/val/test CSVs\n",
        "df_train = pd.read_csv(\"intent_train.csv\")\n",
        "df_val = pd.read_csv(\"intent_val.csv\")\n",
        "df_test = pd.read_csv(\"intent_test.csv\")\n",
        "\n",
        "df_train.to_csv(f\"{preprocessed_folder}/intent_train.csv\", index=False)\n",
        "df_val.to_csv(f\"{preprocessed_folder}/intent_val.csv\", index=False)\n",
        "df_test.to_csv(f\"{preprocessed_folder}/intent_test.csv\", index=False)\n",
        "\n",
        "# ---------------------- BIO ---------------------- #\n",
        "with open(\"bio_train.json\", \"r\") as f: bio_train = json.load(f)\n",
        "with open(\"bio_val.json\", \"r\") as f: bio_val = json.load(f)\n",
        "with open(\"bio_test.json\", \"r\") as f: bio_test = json.load(f)\n",
        "\n",
        "with open(f\"{preprocessed_folder}/bio_train.json\", \"w\") as f: json.dump(bio_train, f, indent=2)\n",
        "with open(f\"{preprocessed_folder}/bio_val.json\", \"w\") as f: json.dump(bio_val, f, indent=2)\n",
        "with open(f\"{preprocessed_folder}/bio_test.json\", \"w\") as f: json.dump(bio_test, f, indent=2)\n",
        "\n",
        "# ---------------------- DST ---------------------- #\n",
        "with open(\"dst_train.json\", \"r\") as f: dst_train = json.load(f)\n",
        "with open(\"dst_val.json\", \"r\") as f: dst_val = json.load(f)\n",
        "with open(\"dst_test.json\", \"r\") as f: dst_test = json.load(f)\n",
        "\n",
        "with open(f\"{preprocessed_folder}/dst_train.json\", \"w\") as f: json.dump(dst_train, f, indent=2)\n",
        "with open(f\"{preprocessed_folder}/dst_val.json\", \"w\") as f: json.dump(dst_val, f, indent=2)\n",
        "with open(f\"{preprocessed_folder}/dst_test.json\", \"w\") as f: json.dump(dst_test, f, indent=2)\n",
        "\n",
        "# ---------------------- Response Generation ---------------------- #\n",
        "with open(\"response_train.json\", \"r\") as f: res_train = json.load(f)\n",
        "with open(\"response_val.json\", \"r\") as f: res_val = json.load(f)\n",
        "with open(\"response_test.json\", \"r\") as f: res_test = json.load(f)\n",
        "\n",
        "with open(f\"{preprocessed_folder}/response_train.json\", \"w\") as f: json.dump(res_train, f, indent=2)\n",
        "with open(f\"{preprocessed_folder}/response_val.json\", \"w\") as f: json.dump(res_val, f, indent=2)\n",
        "with open(f\"{preprocessed_folder}/response_test.json\", \"w\") as f: json.dump(res_test, f, indent=2)\n",
        "\n",
        "print(\"✅ All preprocessed files saved to:\", preprocessed_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukxcosv-Cepa",
        "outputId": "6aa5511d-0fdc-44ee-c0c7-bfe7d15650c9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All preprocessed files saved to: /content/drive/MyDrive/ChatBot/Preprocessed\n"
          ]
        }
      ]
    }
  ]
}